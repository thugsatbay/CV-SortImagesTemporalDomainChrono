{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import time\n",
    "import pickle\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TYNAMO_HOME_DIR = '/nfs/tynamo/home/data/vision7/gdhody/chrono/'\n",
    "BLITZLE_HOME_DIR = '/nfs/blitzle/home/data/vision5/gdhody/chrono/'\n",
    "HOME_DIR = TYNAMO_HOME_DIR\n",
    "HDF5_PATH = os.path.join(HOME_DIR, 'storage.hdf5')\n",
    "PICKLE_PATH = os.path.join(HOME_DIR, 'storage.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL_FILE = os.path.join(HOME_DIR, 'UCF_HMDB_ACT.mat')\n",
    "# PICKLE_PATH = os.path.join(HOME_DIR, 'patch.pkl')\n",
    "# HDF5_PATH = os.path.join(HOME_DIR, 'patch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = sio.loadmat(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_CLASSES = [\n",
    "    'MILITARYPARADE',\n",
    "    'HAIRCUT',\n",
    "    'TAICHI',\n",
    "    'YOYO',\n",
    "    'APPLYEYEMAKEUP',\n",
    "    'BABYCRAWLING',\n",
    "    'PLAYINGTABLA',\n",
    "    'WRITINGONBOARD',\n",
    "    'ROCKCLIMBINGINDOOR',\n",
    "    'BANDMARCHING',\n",
    "    'DRUMMING',\n",
    "    'PLAYINGDAF',\n",
    "    'BLOWINGCANDLES',\n",
    "    'PLAYINGDHOL',\n",
    "    'MOPPINGFLOOR',\n",
    "    'PLAYINGPIANO',\n",
    "    'TYPING',\n",
    "    'SKIJET',\n",
    "    'HEADMASSAGE',\n",
    "    'PLAYINGSITAR',\n",
    "    'HORSERACE',\n",
    "    'SKYDIVING',\n",
    "    'PLAYINGFLUTE',\n",
    "    'APPLYLIPSTICK',\n",
    "    'BRUSHINGTEETH',\n",
    "    'SURFING',\n",
    "    'JUGGLINGBALLS',\n",
    "    'PLAYINGGUITAR',\n",
    "    'SHAVINGBEARD',\n",
    "    'BILLIARDS',\n",
    "    'KNITTING',\n",
    "    'FENCING',\n",
    "    'BOXINGSPEEDBAG',\n",
    "    'MIXING',\n",
    "    'BLOWDRYER',\n",
    "    'SALSASPIN',\n",
    "    'CUTTINGINKITCHEN',\n",
    "    'RAFTING',\n",
    "    'HORSERACE',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_UCF = []\n",
    "TRAIN_HMDB = []\n",
    "for index, sample in enumerate(TRAIN_DATA['filename']):\n",
    "    image_file_name = sample[0][0]\n",
    "    if 'UCF101' in image_file_name:\n",
    "        image_class = image_file_name.split('/')[-1].split('_')[1].upper()\n",
    "        if image_class not in REMOVE_CLASSES:\n",
    "            image_path = os.path.join(HOME_DIR, image_file_name)\n",
    "            if len(glob.glob(image_path)):\n",
    "                frames, crop = TRAIN_DATA['frame'][index], TRAIN_DATA['crop'][index]\n",
    "                TRAIN_UCF.append((image_file_name, np.array(list(crop)), np.array(list(frames))))\n",
    "    if 'HMDB51' in image_file_name:\n",
    "        image_path = os.path.join(HOME_DIR, image_file_name)\n",
    "        if len(glob.glob(image_path)):\n",
    "            frames, crop = TRAIN_DATA['frame'][index], TRAIN_DATA['crop'][index]\n",
    "            TRAIN_HMDB.append((image_file_name, np.array(list(crop)), np.array(list(frames))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174707 89955\n"
     ]
    }
   ],
   "source": [
    "print len(TRAIN_UCF), len(TRAIN_HMDB)\n",
    "random.shuffle(TRAIN_UCF)\n",
    "random.shuffle(TRAIN_HMDB)\n",
    "random.shuffle(TRAIN_UCF)\n",
    "random.shuffle(TRAIN_HMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PICKLE_PATH, 'wb') as f:\n",
    "    pickle.dump([TRAIN_UCF, TRAIN_HMDB], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PICKLE_PATH, 'rb') as f:\n",
    "    TRAIN_UCF, TRAIN_HMDB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_ucf, validation_samples_ucf = (70 * len(TRAIN_UCF)) / 100, len(TRAIN_UCF) / 10\n",
    "test_samples_ucf = len(TRAIN_UCF) - train_samples_ucf - validation_samples_ucf\n",
    "\n",
    "train_samples_hmdb, validation_samples_hmdb = (70 * len(TRAIN_HMDB)) / 100, len(TRAIN_HMDB) / 10\n",
    "test_samples_hmdb = len(TRAIN_HMDB) - train_samples_hmdb - validation_samples_hmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_shape_ucf = (train_samples_ucf, 4, 100, 100, 3)\n",
    "val_shape_ucf = (validation_samples_ucf, 4, 100, 100, 3)\n",
    "test_shape_ucf = (test_samples_ucf, 4, 100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"test_img\": shape (34943, 4, 100, 100, 3), type \"<f4\">"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_file = h5py.File(HDF5_PATH, mode='w')\n",
    "hdf5_file.create_dataset(\"train_img\", train_shape_ucf, np.float32)\n",
    "hdf5_file.create_dataset(\"val_img\", val_shape_ucf, np.float32)\n",
    "hdf5_file.create_dataset(\"test_img\", test_shape_ucf, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10000/34943\n",
      "Train data: 20000/34943\n",
      "Train data: 30000/34943\n",
      "Train data: 40000/34943\n",
      "Train data: 50000/34943\n",
      "Train data: 60000/34943\n",
      "Train data: 70000/34943\n",
      "Train data: 80000/34943\n",
      "Train data: 90000/34943\n",
      "Train data: 100000/34943\n",
      "Train data: 110000/34943\n",
      "Train data: 120000/34943\n"
     ]
    }
   ],
   "source": [
    "image_size, image_padding = 80, 20\n",
    "for index, sample in enumerate(TRAIN_UCF[:train_samples_ucf]):\n",
    "    if index % 10000 == 0 and index > 1:\n",
    "        print 'Train data: {}/{}'.format(index, test_samples_ucf)\n",
    "    im_file_name, crop, frames = sample\n",
    "    im_file_paths = [os.path.join(HOME_DIR, im_file_name, 'Image' + str(frame) + '.jpg') for frame in frames]\n",
    "    images = [Image.open(im_file_path) for im_file_path in im_file_paths]\n",
    "    \n",
    "    top_point = crop[0]\n",
    "    left_point = crop[1]\n",
    "\n",
    "    images = [image.crop((left_point, top_point, \n",
    "                          left_point + image_size + image_padding, \n",
    "                          top_point + image_size + image_padding)) \n",
    "                          for image in images]\n",
    "    \n",
    "    images = [np.array(image, dtype=np.float32) for image in images]\n",
    "    img = np.stack((images[0], images[1], images[2], images[3]), axis=0)\n",
    "    hdf5_file[\"train_img\"][index, ...] = img[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10000/34943\n"
     ]
    }
   ],
   "source": [
    "image_size, image_padding = 80, 20\n",
    "for index, sample in enumerate(TRAIN_UCF[train_samples_ucf:train_samples_ucf + validation_samples_ucf]):\n",
    "    if index % 10000 == 0 and index > 1:\n",
    "        print 'Train data: {}/{}'.format(index, validation_samples_ucf)\n",
    "    im_file_name, crop, frames = sample\n",
    "    im_file_paths = [os.path.join(HOME_DIR, im_file_name, 'Image' + str(frame) + '.jpg') for frame in frames]\n",
    "    images = [Image.open(im_file_path) for im_file_path in im_file_paths]\n",
    "    \n",
    "    top_point = crop[0]\n",
    "    left_point = crop[1]\n",
    "\n",
    "    images = [image.crop((left_point, top_point, \n",
    "                          left_point + image_size + image_padding, \n",
    "                          top_point + image_size + image_padding)) \n",
    "                          for image in images]\n",
    "    \n",
    "    images = [np.array(image, dtype=np.float32) for image in images]\n",
    "    img = np.stack((images[0], images[1], images[2], images[3]), axis=0)\n",
    "    hdf5_file[\"val_img\"][index, ...] = img[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 5000/34943\n",
      "Train data: 10000/34943\n",
      "Train data: 15000/34943\n",
      "Train data: 20000/34943\n",
      "Train data: 25000/34943\n",
      "Train data: 30000/34943\n"
     ]
    }
   ],
   "source": [
    "image_size, image_padding = 80, 20\n",
    "for index, sample in enumerate(TRAIN_UCF[train_samples_ucf + validation_samples_ucf:]):\n",
    "    if index % 5000 == 0 and index > 1:\n",
    "        print 'Train data: {}/{}'.format(index, test_samples_ucf)\n",
    "    im_file_name, crop, frames = sample\n",
    "    im_file_paths = [os.path.join(HOME_DIR, im_file_name, 'Image' + str(frame) + '.jpg') for frame in frames]\n",
    "    images = [Image.open(im_file_path) for im_file_path in im_file_paths]\n",
    "    \n",
    "    top_point = crop[0]\n",
    "    left_point = crop[1]\n",
    "\n",
    "    images = [image.crop((left_point, top_point, \n",
    "                          left_point + image_size + image_padding, \n",
    "                          top_point + image_size + image_padding)) \n",
    "                          for image in images]\n",
    "    \n",
    "    images = [np.array(image, dtype=np.float32) for image in images]\n",
    "    img = np.stack((images[0], images[1], images[2], images[3]), axis=0)\n",
    "    hdf5_file[\"test_img\"][index, ...] = img[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py_tensor]",
   "language": "python",
   "name": "conda-env-py_tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
